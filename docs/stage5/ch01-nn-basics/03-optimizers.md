---
title: "1.3 æ¢¯åº¦ä¸‹é™ä¸ä¼˜åŒ–å™¨"
sidebar_position: 3
description: "æŒæ¡ SGDã€Mini-batchã€Momentumã€Adamã€AdamW ç­‰ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥"
keywords: [ä¼˜åŒ–å™¨, SGD, Adam, AdamW, Momentum, å­¦ä¹ ç‡, å­¦ä¹ ç‡è°ƒåº¦, CosineAnnealing]
---

# æ¢¯åº¦ä¸‹é™ä¸ä¼˜åŒ–å™¨

:::tip æœ¬èŠ‚å®šä½
åœ¨ç¬¬ä¸‰é˜¶æ®µæˆ‘ä»¬å­¦äº†åŸºç¡€æ¢¯åº¦ä¸‹é™ã€‚ç°åœ¨æ·±å…¥äº†è§£æ·±åº¦å­¦ä¹ ä¸­çš„å„ç§ä¼˜åŒ–å™¨â€”â€”**Adam æ˜¯ä½ æœ€å¸¸ç”¨çš„**ï¼Œä½†ç†è§£èƒŒåçš„æ¼”åŒ–é€»è¾‘å¾ˆé‡è¦ã€‚
:::

## å­¦ä¹ ç›®æ ‡

- ç†è§£æ‰¹æ¢¯åº¦ä¸‹é™ã€å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ã€éšæœºæ¢¯åº¦ä¸‹é™çš„åŒºåˆ«
- ç†è§£ Momentum çš„ç›´è§‰
- ğŸ”§ æŒæ¡ Adam / AdamW çš„ä½¿ç”¨
- äº†è§£å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

---

## ä¸€ã€ä¸‰ç§æ¢¯åº¦ä¸‹é™

### 1.1 å¯¹æ¯”

| æ–¹å¼ | æ¯æ¬¡ç”¨å¤šå°‘æ•°æ® | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|-------------|------|------|
| **æ‰¹æ¢¯åº¦ä¸‹é™ï¼ˆBGDï¼‰** | å…¨éƒ¨æ•°æ® | ç¨³å®š | æ…¢ã€å†…å­˜å¤§ |
| **éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰** | 1 ä¸ªæ ·æœ¬ | å¿«ã€èƒ½è·³å‡ºå±€éƒ¨æœ€ä¼˜ | å™ªå£°å¤§ã€ä¸ç¨³å®š |
| **å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMini-batchï¼‰** | ä¸€æ‰¹ï¼ˆ32/64/128ï¼‰ | **å…¼é¡¾é€Ÿåº¦å’Œç¨³å®š** | éœ€é€‰ batch_size |

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
# ç”Ÿæˆæ•°æ®: y = 3x + 2 + noise
X = np.random.randn(200, 1)
y = 3 * X + 2 + np.random.randn(200, 1) * 0.5

def compute_loss(X, y, w, b):
    return np.mean((X * w + b - y) ** 2)

# å¯¹æ¯”ä¸‰ç§æ–¹å¼
methods = {}
for name, batch_size in [('BGD (å…¨é‡)', len(X)), ('SGD (å•æ ·æœ¬)', 1), ('Mini-batch (32)', 32)]:
    w, b = 0.0, 0.0
    lr = 0.05
    losses = []
    for epoch in range(50):
        indices = np.random.permutation(len(X))
        for start in range(0, len(X), batch_size):
            idx = indices[start:start+batch_size]
            X_batch, y_batch = X[idx], y[idx]
            pred = X_batch * w + b
            grad_w = 2 * np.mean(X_batch * (pred - y_batch))
            grad_b = 2 * np.mean(pred - y_batch)
            w -= lr * grad_w
            b -= lr * grad_b
        losses.append(compute_loss(X, y, w, b))
    methods[name] = losses

for name, losses in methods.items():
    plt.plot(losses, label=name, linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('ä¸‰ç§æ¢¯åº¦ä¸‹é™å¯¹æ¯”')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

## äºŒã€Momentumâ€”â€”å¸¦æƒ¯æ€§çš„ä¸‹é™

### 2.1 ç›´è§‰

æƒ³è±¡ä¸€ä¸ªçƒä»å±±å¡æ»šä¸‹æ¥ã€‚æ™®é€š SGD æ¯ä¸€æ­¥åªçœ‹å½“å‰æ¢¯åº¦æ–¹å‘ã€‚Momentum è®©çƒ**å¸¦ä¸Šæƒ¯æ€§**â€”â€”å³ä½¿é‡åˆ°å°å‘ä¹Ÿèƒ½æ»‘è¿‡å»ã€‚

> **v = Î² Ã— v + (1-Î²) Ã— gradient**
>
> **w = w - lr Ã— v**

```python
# å¯¹æ¯” SGD å’Œ Momentum
def optimize_2d(optimizer_fn, steps=100):
    """åœ¨ f(x,y) = xÂ² + 10yÂ² ä¸Šä¼˜åŒ–"""
    x, y = np.array(5.0), np.array(5.0)
    path = [(x, y)]
    state = {}
    for _ in range(steps):
        gx, gy = 2*x, 20*y  # æ¢¯åº¦
        x, y, state = optimizer_fn(x, y, gx, gy, state)
        path.append((x, y))
    return np.array(path)

def sgd(x, y, gx, gy, state, lr=0.05):
    return x - lr*gx, y - lr*gy, state

def momentum(x, y, gx, gy, state, lr=0.05, beta=0.9):
    vx = state.get('vx', 0)
    vy = state.get('vy', 0)
    vx = beta * vx + gx
    vy = beta * vy + gy
    state['vx'], state['vy'] = vx, vy
    return x - lr*vx, y - lr*vy, state

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
for ax, (name, fn) in zip(axes, [('SGD', sgd), ('Momentum', momentum)]):
    path = optimize_2d(fn, 50)
    # ç­‰é«˜çº¿
    xx, yy = np.meshgrid(np.linspace(-6, 6, 100), np.linspace(-6, 6, 100))
    zz = xx**2 + 10*yy**2
    ax.contour(xx, yy, zz, levels=20, cmap='Blues', alpha=0.5)
    ax.plot(path[:, 0], path[:, 1], 'ro-', markersize=3, linewidth=1)
    ax.set_title(name)
    ax.set_xlim(-6, 6)
    ax.set_ylim(-6, 6)
plt.suptitle('SGD vs Momentum ä¼˜åŒ–è·¯å¾„', fontsize=13)
plt.tight_layout()
plt.show()
```

---

## ä¸‰ã€Adamâ€”â€”æœ€å¸¸ç”¨çš„ä¼˜åŒ–å™¨

### 3.1 æ ¸å¿ƒæ€æƒ³

Adam ç»“åˆäº† Momentumï¼ˆä¸€é˜¶åŠ¨é‡ï¼‰å’Œ RMSPropï¼ˆäºŒé˜¶åŠ¨é‡ï¼‰ï¼š
- **ä¸€é˜¶åŠ¨é‡ m**ï¼šæ¢¯åº¦çš„ç§»åŠ¨å¹³å‡ï¼ˆæ–¹å‘ï¼‰
- **äºŒé˜¶åŠ¨é‡ v**ï¼šæ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡ï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰

### 3.2 PyTorch ä¸­ä½¿ç”¨

```python
import torch
import torch.nn as nn

# ç”¨ PyTorch å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨
model_configs = {
    'SGD': lambda params: torch.optim.SGD(params, lr=0.01),
    'SGD+Momentum': lambda params: torch.optim.SGD(params, lr=0.01, momentum=0.9),
    'Adam': lambda params: torch.optim.Adam(params, lr=0.01),
    'AdamW': lambda params: torch.optim.AdamW(params, lr=0.01, weight_decay=0.01),
}

# ç®€å•ä»»åŠ¡: æ‹Ÿåˆ y = sin(x)
torch.manual_seed(42)
X = torch.linspace(-3, 3, 200).unsqueeze(1)
y = torch.sin(X)

results = {}
for name, opt_fn in model_configs.items():
    model = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))
    optimizer = opt_fn(model.parameters())
    criterion = nn.MSELoss()
    losses = []

    for epoch in range(300):
        pred = model(X)
        loss = criterion(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        losses.append(loss.item())

    results[name] = losses

plt.figure(figsize=(10, 5))
for name, losses in results.items():
    plt.plot(losses, label=name, linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('ä¸åŒä¼˜åŒ–å™¨æ”¶æ•›é€Ÿåº¦å¯¹æ¯”')
plt.legend()
plt.yscale('log')
plt.grid(True, alpha=0.3)
plt.show()
```

### 3.3 ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—

| ä¼˜åŒ–å™¨ | ç‰¹ç‚¹ | ä½¿ç”¨åœºæ™¯ |
|--------|------|---------|
| **SGD** | ç®€å•ã€éœ€è°ƒå­¦ä¹ ç‡ | ç ”ç©¶å®éªŒ |
| **SGD+Momentum** | åŠ é€Ÿæ”¶æ•› | CV ç»å…¸æ¨¡å‹ |
| **Adam** | è‡ªé€‚åº”å­¦ä¹ ç‡ã€å¿«é€Ÿæ”¶æ•› | **é»˜è®¤é¦–é€‰** |
| **AdamW** | Adam + è§£è€¦æƒé‡è¡°å‡ | **Transformerã€å¤§æ¨¡å‹** |
| **RMSProp** | è‡ªé€‚åº”å­¦ä¹ ç‡ | RNN |

:::info Adam vs AdamW
Adam æŠŠ L2 æ­£åˆ™åŒ–æ··åœ¨æ¢¯åº¦é‡Œã€‚AdamW æŠŠæƒé‡è¡°å‡å•ç‹¬åšï¼Œæ•ˆæœæ›´å¥½ã€‚**ç°åœ¨å¤§å¤šæ•°æƒ…å†µç”¨ AdamWã€‚**
:::

---

## å››ã€å­¦ä¹ ç‡è°ƒåº¦

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ

å›ºå®šå­¦ä¹ ç‡æœ‰é—®é¢˜ï¼šå¤ªå¤§ â†’ ä¸æ”¶æ•›ï¼›å¤ªå° â†’ å¤ªæ…¢ã€‚**å­¦ä¹ ç‡è°ƒåº¦**è®©å­¦ä¹ ç‡éšè®­ç»ƒåŠ¨æ€è°ƒæ•´ã€‚

### 4.2 å¸¸ç”¨ç­–ç•¥

```python
import torch.optim.lr_scheduler as lr_scheduler

model = nn.Linear(10, 1)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

schedulers = {
    'StepLR (æ¯30æ­¥Ã—0.1)': lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1),
    'CosineAnnealing': lr_scheduler.CosineAnnealingLR(optimizer, T_max=100),
}

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
for ax, (name, scheduler) in zip(axes, schedulers.items()):
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    if 'Step' in name:
        scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    else:
        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

    lrs = []
    for epoch in range(100):
        lrs.append(optimizer.param_groups[0]['lr'])
        optimizer.step()
        scheduler.step()

    ax.plot(lrs, linewidth=2, color='steelblue')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Learning Rate')
    ax.set_title(name)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 4.3 Warmup

å…ˆç”¨å°å­¦ä¹ ç‡é¢„çƒ­å‡ æ­¥ï¼Œå†é€æ¸å¢å¤§åˆ°æ­£å¸¸å€¼ï¼Œæœ€åç¼“æ…¢é™ä½ã€‚**Transformer è®­ç»ƒçš„æ ‡é…ã€‚**

| ç­–ç•¥ | è¯´æ˜ | å¸¸ç”¨åœºæ™¯ |
|------|------|---------|
| **StepLR** | æ¯ N æ­¥ä¹˜ä»¥ Î³ | ç®€å•ä»»åŠ¡ |
| **CosineAnnealing** | ä½™å¼¦æ›²çº¿è¡°å‡ | CNN è®­ç»ƒ |
| **Warmup + Cosine** | å…ˆå‡åé™ | **Transformer** |
| **ReduceLROnPlateau** | éªŒè¯é›†ä¸é™æ—¶å‡ | è‡ªé€‚åº” |

---

## äº”ã€å°ç»“

| æ¦‚å¿µ | è¦ç‚¹ |
|------|------|
| Mini-batch SGD | å®é™…è®­ç»ƒä¸­æœ€å¸¸ç”¨çš„æ¢¯åº¦è®¡ç®—æ–¹å¼ |
| Momentum | ç»™æ¢¯åº¦åŠ ä¸Šæƒ¯æ€§ï¼ŒåŠ é€Ÿæ”¶æ•› |
| Adam / AdamW | è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œ**é¦–é€‰ä¼˜åŒ–å™¨** |
| å­¦ä¹ ç‡è°ƒåº¦ | è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ |

---

## åŠ¨æ‰‹ç»ƒä¹ 

### ç»ƒä¹  1ï¼šä¼˜åŒ–å™¨èµ›é©¬

ç”¨ `make_moons` æ•°æ®é›†ï¼Œè®­ç»ƒä¸€ä¸ª MLPï¼ˆPyTorchï¼‰ï¼Œå¯¹æ¯” SGDã€SGD+Momentumã€Adamã€AdamW çš„æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆå‡†ç¡®ç‡ã€‚

### ç»ƒä¹  2ï¼šå­¦ä¹ ç‡æ•æ„Ÿæ€§

ç”¨ Adam è®­ç»ƒåŒä¸€ä¸ªæ¨¡å‹ï¼Œæµ‹è¯•å­¦ä¹ ç‡ 0.1, 0.01, 0.001, 0.0001 çš„æ•ˆæœï¼Œç”»å‡ºå­¦ä¹ æ›²çº¿å¯¹æ¯”ã€‚
