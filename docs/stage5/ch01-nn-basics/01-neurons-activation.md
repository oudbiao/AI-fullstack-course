---
title: "1.1 ä»ç¥ç»å…ƒåˆ°å¤šå±‚æ„ŸçŸ¥æœº"
sidebar_position: 1
description: "ç†è§£äººå·¥ç¥ç»å…ƒã€æ„ŸçŸ¥æœºæ¨¡å‹å’Œå¸¸ç”¨æ¿€æ´»å‡½æ•°ï¼Œæ­å»ºä½ çš„ç¬¬ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰"
keywords: [ç¥ç»å…ƒ, æ„ŸçŸ¥æœº, æ¿€æ´»å‡½æ•°, ReLU, Sigmoid, Tanh, MLP, å¤šå±‚æ„ŸçŸ¥æœº]
---

# ä»ç¥ç»å…ƒåˆ°å¤šå±‚æ„ŸçŸ¥æœº

:::tip æœ¬èŠ‚å®šä½
æ·±åº¦å­¦ä¹ çš„ä¸€åˆ‡éƒ½ä»**äººå·¥ç¥ç»å…ƒ**å¼€å§‹ã€‚æœ¬èŠ‚ä»æœ€ç®€å•çš„æ„ŸçŸ¥æœºå‡ºå‘ï¼Œè®¤è¯†å„ç§æ¿€æ´»å‡½æ•°ï¼Œå†ç»„è£…æˆå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰â€”â€”è¿™æ˜¯æ‰€æœ‰ç¥ç»ç½‘ç»œçš„åŸºç¡€ã€‚
:::

## å­¦ä¹ ç›®æ ‡

- ç†è§£ä»ç”Ÿç‰©ç¥ç»å…ƒåˆ°äººå·¥ç¥ç»å…ƒçš„æ˜ å°„
- æŒæ¡æ„ŸçŸ¥æœºæ¨¡å‹
- æŒæ¡å¸¸ç”¨æ¿€æ´»å‡½æ•°ï¼šReLUã€Sigmoidã€Tanh ç­‰
- ç†è§£å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„ç»“æ„

---

## ä¸€ã€ä»ç”Ÿç‰©åˆ°äººå·¥

```mermaid
flowchart LR
    subgraph BIO["ç”Ÿç‰©ç¥ç»å…ƒ"]
        D["æ ‘çª<br/>æ¥æ”¶ä¿¡å·"] --> S["ç»†èƒä½“<br/>æ±‡æ€»åŠ å·¥"]
        S --> A["è½´çª<br/>è¾“å‡ºä¿¡å·"]
    end
    subgraph ART["äººå·¥ç¥ç»å…ƒ"]
        X["è¾“å…¥ x1, x2, ..."] --> W["åŠ æƒæ±‚å’Œ<br/>z = w1Â·x1 + w2Â·x2 + b"]
        W --> ACT["æ¿€æ´»å‡½æ•°<br/>a = f(z)"]
        ACT --> O["è¾“å‡º"]
    end

    style D fill:#e3f2fd,stroke:#1565c0,color:#333
    style S fill:#e3f2fd,stroke:#1565c0,color:#333
    style A fill:#e3f2fd,stroke:#1565c0,color:#333
    style X fill:#fff3e0,stroke:#e65100,color:#333
    style W fill:#fff3e0,stroke:#e65100,color:#333
    style ACT fill:#fff3e0,stroke:#e65100,color:#333
    style O fill:#fff3e0,stroke:#e65100,color:#333
```

æ ¸å¿ƒå¯¹åº”å…³ç³»ï¼š

| ç”Ÿç‰© | äººå·¥ |
|------|------|
| æ ‘çªï¼ˆæ¥æ”¶ä¿¡å·ï¼‰ | è¾“å…¥ x |
| çªè§¦å¼ºåº¦ | æƒé‡ w |
| ç»†èƒä½“ï¼ˆæ±‡æ€»ï¼‰ | åŠ æƒæ±‚å’Œ z = Î£(wiÂ·xi) + b |
| æ¿€æ´»/æŠ‘åˆ¶ | æ¿€æ´»å‡½æ•° f(z) |
| è½´çªï¼ˆè¾“å‡ºï¼‰ | è¾“å‡º a = f(z) |

---

## äºŒã€æ„ŸçŸ¥æœºâ€”â€”æœ€ç®€å•çš„äººå·¥ç¥ç»å…ƒ

### 2.1 æ¨¡å‹

æ„ŸçŸ¥æœºæ˜¯ä¸€ä¸ªåš**äºŒåˆ†ç±»**çš„ç®€å•æ¨¡å‹ï¼š

> **z = w1Â·x1 + w2Â·x2 + ... + wnÂ·xn + b**
>
> **è¾“å‡º = 1 å¦‚æœ z > 0ï¼Œå¦åˆ™ = 0**

```python
import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    """æœ€ç®€å•çš„æ„ŸçŸ¥æœº"""
    def __init__(self, n_features, lr=0.1):
        self.w = np.zeros(n_features)
        self.b = 0
        self.lr = lr

    def predict(self, x):
        z = np.dot(x, self.w) + self.b
        return 1 if z > 0 else 0

    def train(self, X, y, epochs=20):
        for epoch in range(epochs):
            errors = 0
            for xi, yi in zip(X, y):
                pred = self.predict(xi)
                error = yi - pred
                if error != 0:
                    self.w += self.lr * error * xi
                    self.b += self.lr * error
                    errors += 1
            if errors == 0:
                print(f"ç¬¬ {epoch+1} è½®æ”¶æ•›ï¼")
                break

# AND é—¨
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([0, 0, 0, 1])

p = Perceptron(2)
p.train(X, y)
print(f"æƒé‡: {p.w}, åç½®: {p.b}")
for xi, yi in zip(X, y):
    print(f"  è¾“å…¥ {xi} â†’ é¢„æµ‹ {p.predict(xi)}, çœŸå® {yi}")
```

### 2.2 æ„ŸçŸ¥æœºçš„å±€é™

æ„ŸçŸ¥æœºåªèƒ½è§£å†³**çº¿æ€§å¯åˆ†**é—®é¢˜ã€‚XOR é—®é¢˜å°±æ— æ³•è§£å†³â€”â€”è¿™æ­£æ˜¯å¤šå±‚ç½‘ç»œå‡ºç°çš„åŸå› ã€‚

```python
# XOR é—®é¢˜â€”â€”æ„ŸçŸ¥æœºæ— æ³•è§£å†³
X_xor = np.array([[0,0], [0,1], [1,0], [1,1]])
y_xor = np.array([0, 1, 1, 0])

p_xor = Perceptron(2)
p_xor.train(X_xor, y_xor, epochs=100)

print("\nXOR é¢„æµ‹ç»“æœ:")
for xi, yi in zip(X_xor, y_xor):
    print(f"  è¾“å…¥ {xi} â†’ é¢„æµ‹ {p_xor.predict(xi)}, çœŸå® {yi}")
```

---

## ä¸‰ã€æ¿€æ´»å‡½æ•°

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ

å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œå¤šå±‚ç½‘ç»œå°±é€€åŒ–ä¸ºä¸€ä¸ªçº¿æ€§æ¨¡å‹â€”â€”æ— è®ºå å¤šå°‘å±‚ï¼Œæ•ˆæœç­‰åŒäºå•å±‚ã€‚æ¿€æ´»å‡½æ•°å¼•å…¥**éçº¿æ€§**ï¼Œè®©ç½‘ç»œèƒ½æ‹Ÿåˆä»»æ„å¤æ‚çš„å‡½æ•°ã€‚

### 3.2 å¸¸ç”¨æ¿€æ´»å‡½æ•°

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-5, 5, 200)

# å„ç§æ¿€æ´»å‡½æ•°
activations = {
    'Sigmoid': (1 / (1 + np.exp(-x)), 'Ïƒ(x) = 1/(1+eâ»Ë£)'),
    'Tanh': (np.tanh(x), 'tanh(x)'),
    'ReLU': (np.maximum(0, x), 'max(0, x)'),
    'Leaky ReLU': (np.where(x > 0, x, 0.01 * x), 'max(0.01x, x)'),
}

fig, axes = plt.subplots(2, 2, figsize=(12, 8))
colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']

for ax, (name, (y, formula)), color in zip(axes.ravel(), activations.items(), colors):
    ax.plot(x, y, linewidth=2, color=color)
    ax.axhline(0, color='gray', linewidth=0.5)
    ax.axvline(0, color='gray', linewidth=0.5)
    ax.set_title(f'{name}: {formula}', fontsize=12)
    ax.set_xlim(-5, 5)
    ax.grid(True, alpha=0.3)

plt.suptitle('å¸¸ç”¨æ¿€æ´»å‡½æ•°', fontsize=14)
plt.tight_layout()
plt.show()
```

### 3.3 å¯¹æ¯”ä¸é€‰æ‹©

| æ¿€æ´»å‡½æ•° | è¾“å‡ºèŒƒå›´ | ä¼˜ç‚¹ | ç¼ºç‚¹ | ä½¿ç”¨åœºæ™¯ |
|---------|---------|------|------|---------|
| **ReLU** | [0, +âˆ) | è®¡ç®—å¿«ã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤± | ç¥ç»å…ƒ"æ­»äº¡" | **éšè—å±‚é¦–é€‰** |
| **Sigmoid** | (0, 1) | è¾“å‡ºæ¦‚ç‡è§£é‡Š | æ¢¯åº¦æ¶ˆå¤±ã€éé›¶ä¸­å¿ƒ | äºŒåˆ†ç±»è¾“å‡ºå±‚ |
| **Tanh** | (-1, 1) | é›¶ä¸­å¿ƒ | æ¢¯åº¦æ¶ˆå¤± | RNNï¼ˆè¾ƒå°‘ç”¨ï¼‰ |
| **Leaky ReLU** | (-âˆ, +âˆ) | é¿å…ç¥ç»å…ƒæ­»äº¡ | å¤šä¸€ä¸ªè¶…å‚æ•° | ReLU æ”¹è¿› |
| **GELU** | çº¦ (-0.17, +âˆ) | å¹³æ»‘ã€æ•ˆæœå¥½ | è®¡ç®—ç¨æ…¢ | Transformer |
| **Swish** | çº¦ (-0.28, +âˆ) | å¹³æ»‘ã€è‡ªé—¨æ§ | è®¡ç®—ç¨æ…¢ | æ–°æ¶æ„ |

:::info ReLU çš„"ç¥ç»å…ƒæ­»äº¡"
å½“è¾“å…¥å§‹ç»ˆä¸ºè´Ÿæ—¶ï¼ŒReLU è¾“å‡ºæ°¸è¿œä¸º 0ï¼Œæ¢¯åº¦ä¹Ÿä¸º 0ï¼Œå‚æ•°ä¸å†æ›´æ–°ã€‚Leaky ReLU é€šè¿‡ç»™è´Ÿæ•°ä¸€ä¸ªå°æ–œç‡ï¼ˆ0.01ï¼‰æ¥ç¼“è§£ã€‚
:::

---

## å››ã€å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

### 4.1 ç»“æ„

æŠŠå¤šä¸ªç¥ç»å…ƒ**æŒ‰å±‚æ’åˆ—**ï¼Œå‰ä¸€å±‚çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼š

```mermaid
flowchart LR
    subgraph INPUT["è¾“å…¥å±‚"]
        I1["x1"]
        I2["x2"]
        I3["x3"]
    end
    subgraph HIDDEN["éšè—å±‚"]
        H1["h1"]
        H2["h2"]
        H3["h3"]
        H4["h4"]
    end
    subgraph OUTPUT["è¾“å‡ºå±‚"]
        O1["y1"]
        O2["y2"]
    end

    I1 --> H1 & H2 & H3 & H4
    I2 --> H1 & H2 & H3 & H4
    I3 --> H1 & H2 & H3 & H4
    H1 --> O1 & O2
    H2 --> O1 & O2
    H3 --> O1 & O2
    H4 --> O1 & O2

    style INPUT fill:#e3f2fd,stroke:#1565c0,color:#333
    style HIDDEN fill:#fff3e0,stroke:#e65100,color:#333
    style OUTPUT fill:#e8f5e9,stroke:#2e7d32,color:#333
```

### 4.2 ç”¨ NumPy å®ç° MLP è§£å†³ XOR

```python
np.random.seed(42)

# XOR æ•°æ®
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0], [1], [1], [0]])

# ç½‘ç»œ: 2 â†’ 4 â†’ 1
W1 = np.random.randn(2, 4) * 0.5
b1 = np.zeros((1, 4))
W2 = np.random.randn(4, 1) * 0.5
b2 = np.zeros((1, 1))

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_deriv(a):
    return a * (1 - a)

lr = 1.0
losses = []

for epoch in range(5000):
    # å‰å‘ä¼ æ’­
    z1 = X @ W1 + b1
    a1 = sigmoid(z1)
    z2 = a1 @ W2 + b2
    a2 = sigmoid(z2)

    # æŸå¤±
    loss = np.mean((y - a2) ** 2)
    losses.append(loss)

    # åå‘ä¼ æ’­
    dz2 = (a2 - y) * sigmoid_deriv(a2)
    dW2 = a1.T @ dz2 / 4
    db2 = np.mean(dz2, axis=0, keepdims=True)

    dz1 = (dz2 @ W2.T) * sigmoid_deriv(a1)
    dW1 = X.T @ dz1 / 4
    db1 = np.mean(dz1, axis=0, keepdims=True)

    # æ›´æ–°
    W2 -= lr * dW2
    b2 -= lr * db2
    W1 -= lr * dW1
    b1 -= lr * db1

print(f"æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}")
print("XOR é¢„æµ‹:")
for xi, yi, pred in zip(X, y, a2):
    print(f"  {xi} â†’ {pred[0]:.4f}, çœŸå® {yi[0]}")

plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('MLP è§£å†³ XOR')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## äº”ã€å°ç»“

| æ¦‚å¿µ | è¦ç‚¹ |
|------|------|
| äººå·¥ç¥ç»å…ƒ | åŠ æƒæ±‚å’Œ + æ¿€æ´»å‡½æ•° |
| æ„ŸçŸ¥æœº | æœ€ç®€å•çš„ç¥ç»å…ƒï¼Œåªèƒ½çº¿æ€§åˆ†ç±» |
| æ¿€æ´»å‡½æ•° | å¼•å…¥éçº¿æ€§ï¼›éšè—å±‚ç”¨ ReLU |
| MLP | å¤šå±‚å †å ï¼Œå¯æ‹Ÿåˆä»»æ„å‡½æ•° |

```mermaid
mindmap
  root((ç¥ç»ç½‘ç»œåŸºç¡€))
    äººå·¥ç¥ç»å…ƒ
      åŠ æƒæ±‚å’Œ
      åç½®
      æ¿€æ´»å‡½æ•°
    æ¿€æ´»å‡½æ•°
      ReLU ğŸ”§é¦–é€‰
      Sigmoid è¾“å‡ºå±‚
      Tanh é›¶ä¸­å¿ƒ
      GELU Transformer
    MLP
      è¾“å…¥å±‚
      éšè—å±‚
      è¾“å‡ºå±‚
      èƒ½è§£å†³ XOR
```

---

## åŠ¨æ‰‹ç»ƒä¹ 

### ç»ƒä¹  1ï¼šå®ç° OR é—¨æ„ŸçŸ¥æœº

ä¿®æ”¹ AND é—¨çš„è®­ç»ƒæ•°æ®ä¸º OR é—¨ï¼ˆ0|0â†’0, 0|1â†’1, 1|0â†’1, 1|1â†’1ï¼‰ï¼Œè®­ç»ƒæ„ŸçŸ¥æœºå¹¶ç”»å‡ºå†³ç­–è¾¹ç•Œã€‚

### ç»ƒä¹  2ï¼šMLP åˆ†ç±»æœˆç‰™æ•°æ®

ç”¨ `sklearn.datasets.make_moons` ç”Ÿæˆæœˆç‰™æ•°æ®ï¼Œæ‰‹å†™ NumPy MLPï¼ˆ2â†’8â†’1ï¼‰ï¼Œè®­ç»ƒåç”»å‡ºå†³ç­–è¾¹ç•Œã€‚
