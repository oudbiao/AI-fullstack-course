---
title: "1.4 æ·±åº¦å­¦ä¹ ä¸­çš„æ­£åˆ™åŒ–"
sidebar_position: 4
description: "æŒæ¡ Dropoutã€Batch Normalizationã€Layer Normalizationã€æ•°æ®å¢å¼ºå’Œæ—©åœæ³•"
keywords: [æ­£åˆ™åŒ–, Dropout, Batch Normalization, Layer Normalization, æ•°æ®å¢å¼º, Early Stopping]
---

# æ·±åº¦å­¦ä¹ ä¸­çš„æ­£åˆ™åŒ–

:::tip æœ¬èŠ‚å®šä½
æ·±åº¦ç½‘ç»œå‚æ•°é‡å·¨å¤§ï¼Œéå¸¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚æœ¬èŠ‚ä»‹ç»æ·±åº¦å­¦ä¹ ç‰¹æœ‰çš„æ­£åˆ™åŒ–æŠ€æœ¯â€”â€”**Dropout å’Œ BatchNorm æ˜¯ä½ å¿…é¡»æŒæ¡çš„ä¸¤ä¸ªã€‚**
:::

## å­¦ä¹ ç›®æ ‡

- ğŸ”§ æŒæ¡ Dropout çš„åŸç†å’Œä½¿ç”¨
- ğŸ”§ æŒæ¡ Batch Normalizationï¼ˆBNï¼‰
- ç†è§£ Layer Normalizationï¼ˆLNï¼‰
- ğŸ”§ æŒæ¡æ•°æ®å¢å¼ºå’Œæ—©åœæ³•

---

## ä¸€ã€å›é¡¾ï¼šL1/L2 æ­£åˆ™åŒ–

ç¬¬å››é˜¶æ®µå·²å­¦è¿‡â€”â€”L2 æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰åœ¨æ·±åº¦å­¦ä¹ ä¸­ç›´æ¥é€šè¿‡ä¼˜åŒ–å™¨çš„ `weight_decay` å‚æ•°ä½¿ç”¨ï¼š

```python
import torch
import torch.nn as nn

# AdamW è‡ªå¸¦æƒé‡è¡°å‡
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
```

---

## äºŒã€Dropoutâ€”â€”éšæœºä¸¢å¼ƒ

### 2.1 åŸç†

è®­ç»ƒæ—¶ï¼Œ**éšæœºè®©ä¸€éƒ¨åˆ†ç¥ç»å…ƒä¸å·¥ä½œ**ï¼ˆè¾“å‡ºç½®ä¸º 0ï¼‰ã€‚è¿™è¿«ä½¿ç½‘ç»œä¸ä¾èµ–ä»»ä½•å•ä¸ªç¥ç»å…ƒï¼Œå¢å¼ºé²æ£’æ€§ã€‚

```mermaid
flowchart LR
    subgraph TRAIN["è®­ç»ƒæ—¶ (Dropout=0.5)"]
        A1["h1"] --> O1["è¾“å‡º"]
        A2["h2 âŒ"] -.-> O1
        A3["h3"] --> O1
        A4["h4 âŒ"] -.-> O1
    end
    subgraph TEST["æ¨ç†æ—¶ï¼ˆå…¨éƒ¨å‚ä¸ï¼‰"]
        B1["h1"] --> O2["è¾“å‡º"]
        B2["h2"] --> O2
        B3["h3"] --> O2
        B4["h4"] --> O2
    end

    style TRAIN fill:#fff3e0,stroke:#e65100,color:#333
    style TEST fill:#e8f5e9,stroke:#2e7d32,color:#333
```

### 2.2 PyTorch ä½¿ç”¨

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

# æ•°æ®
X, y = make_moons(500, noise=0.3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_train_t = torch.FloatTensor(X_train)
y_train_t = torch.LongTensor(y_train)
X_test_t = torch.FloatTensor(X_test)
y_test_t = torch.LongTensor(y_test)

# å¯¹æ¯”æœ‰æ—  Dropout
class MLP(nn.Module):
    def __init__(self, dropout_rate=0.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 64),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(64, 2),
        )

    def forward(self, x):
        return self.net(x)

results = {}
for name, drop in [('æ—  Dropout', 0.0), ('Dropout=0.3', 0.3), ('Dropout=0.5', 0.5)]:
    model = MLP(drop)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()
    train_losses, test_losses = [], []

    for epoch in range(200):
        model.train()
        loss = criterion(model(X_train_t), y_train_t)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

        model.eval()
        with torch.no_grad():
            test_loss = criterion(model(X_test_t), y_test_t)
            test_losses.append(test_loss.item())

    results[name] = (train_losses, test_losses)

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for ax, (name, (tr, te)) in zip(axes, results.items()):
    ax.plot(tr, label='è®­ç»ƒ', linewidth=2)
    ax.plot(te, label='æµ‹è¯•', linewidth=2)
    ax.set_title(name)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
plt.suptitle('Dropout å¯¹è¿‡æ‹Ÿåˆçš„å½±å“', fontsize=13)
plt.tight_layout()
plt.show()
```

:::info é‡è¦
- `model.train()` å¼€å¯ Dropout
- `model.eval()` å…³é—­ Dropout
- **æ¨ç†æ—¶ä¸€å®šè¦è°ƒ `model.eval()`ï¼**
:::

---

## ä¸‰ã€Batch Normalizationï¼ˆBNï¼‰

### 3.1 åŸç†

å¯¹æ¯ä¸€å±‚çš„è¾“å‡ºåš**å½’ä¸€åŒ–**ï¼ˆå‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ï¼‰ï¼Œç„¶åç”¨å¯å­¦ä¹ çš„å‚æ•°ç¼©æ”¾å’Œå¹³ç§»ã€‚

**ä½œç”¨ï¼š**
- åŠ é€Ÿæ”¶æ•›
- å‡å°‘å¯¹åˆå§‹åŒ–çš„æ•æ„Ÿæ€§
- æœ‰è½»å¾®æ­£åˆ™åŒ–æ•ˆæœ

### 3.2 PyTorch ä½¿ç”¨

```python
class MLP_BN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 64),
            nn.BatchNorm1d(64),   # BN æ”¾åœ¨æ¿€æ´»å‡½æ•°å‰é¢
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Linear(64, 2),
        )

    def forward(self, x):
        return self.net(x)

# å¯¹æ¯”æœ‰æ—  BN
for name, ModelClass in [('æ—  BN', MLP), ('æœ‰ BN', MLP_BN)]:
    model = ModelClass() if name == 'æœ‰ BN' else ModelClass(0.0)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # ç”¨ SGD æ›´æ˜æ˜¾
    criterion = nn.CrossEntropyLoss()

    for epoch in range(100):
        model.train()
        loss = criterion(model(X_train_t), y_train_t)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    model.eval()
    with torch.no_grad():
        acc = (model(X_test_t).argmax(1) == y_test_t).float().mean()
    print(f"{name}: æµ‹è¯•å‡†ç¡®ç‡ = {acc:.4f}")
```

---

## å››ã€Layer Normalizationï¼ˆLNï¼‰

### BN vs LN

| ç‰¹æ€§ | Batch Normalization | Layer Normalization |
|------|-------------------|-------------------|
| å½’ä¸€åŒ–ç»´åº¦ | è·¨æ ·æœ¬ï¼ˆbatch ç»´ï¼‰ | è·¨ç‰¹å¾ï¼ˆlayer ç»´ï¼‰ |
| ä¾èµ– batch size | æ˜¯ | å¦ |
| é€‚ç”¨ | **CNN** | **Transformerã€RNN** |

```python
# BN vs LN ä½¿ç”¨
bn = nn.BatchNorm1d(64)    # è¾“å…¥: (batch, 64)
ln = nn.LayerNorm(64)      # è¾“å…¥: (batch, 64)

x = torch.randn(32, 64)
print(f"BN è¾“å‡ºå½¢çŠ¶: {bn(x).shape}")
print(f"LN è¾“å‡ºå½¢çŠ¶: {ln(x).shape}")
```

:::info
è®°ä½ï¼š**CNN ç”¨ BNï¼ŒTransformer ç”¨ LNã€‚** è¿™æ˜¯å®é™…å·¥ç¨‹ä¸­çš„æ ‡å‡†é€‰æ‹©ã€‚
:::

---

## äº”ã€æ•°æ®å¢å¼º

### 5.1 å›¾åƒæ•°æ®å¢å¼º

```python
from torchvision import transforms

# å¸¸ç”¨çš„å›¾åƒå¢å¼ºç»„åˆ
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),     # éšæœºæ°´å¹³ç¿»è½¬
    transforms.RandomRotation(15),               # éšæœºæ—‹è½¬ Â±15Â°
    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # é¢œè‰²æ‰°åŠ¨
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),   # éšæœºè£å‰ª
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# æµ‹è¯•é›†ä¸åšå¢å¼º
test_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
```

---

## å…­ã€æ—©åœæ³•ï¼ˆEarly Stoppingï¼‰

### 6.1 åŸç†

ç›‘æ§**éªŒè¯é›†æŸå¤±**ï¼Œè¿ç»­ N è½®ä¸ä¸‹é™å°±åœæ­¢è®­ç»ƒã€‚

```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        self.should_stop = False

    def step(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
        return self.should_stop

# ä½¿ç”¨ç¤ºä¾‹
early_stop = EarlyStopping(patience=10)
# for epoch in range(max_epochs):
#     train(...)
#     val_loss = validate(...)
#     if early_stop.step(val_loss):
#         print(f"æ—©åœ! Epoch {epoch}")
#         break
```

---

## ä¸ƒã€å°ç»“

| æŠ€æœ¯ | ç±»å‹ | è¦ç‚¹ |
|------|------|------|
| **Dropout** | é˜²è¿‡æ‹Ÿåˆ | è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒï¼Œæ¨ç†æ—¶å…³é—­ |
| **Batch Norm** | åŠ é€Ÿ+æ­£åˆ™ | CNN æ ‡é…ï¼Œæ”¾åœ¨æ¿€æ´»å‰ |
| **Layer Norm** | åŠ é€Ÿ+æ­£åˆ™ | Transformer æ ‡é… |
| **æ•°æ®å¢å¼º** | å¢åŠ å¤šæ ·æ€§ | åªåœ¨è®­ç»ƒé›†ç”¨ |
| **æ—©åœæ³•** | é˜²è¿‡æ‹Ÿåˆ | ç›‘æ§éªŒè¯é›† loss |
| **æƒé‡è¡°å‡** | L2 æ­£åˆ™ | optimizer çš„ weight_decay |

---

## åŠ¨æ‰‹ç»ƒä¹ 

### ç»ƒä¹  1ï¼šæ­£åˆ™åŒ–ç»„åˆ

åœ¨ MNIST æ•°æ®é›†ä¸Šè®­ç»ƒ MLPï¼Œä¾æ¬¡æ·»åŠ  Dropoutã€BatchNormã€æ•°æ®å¢å¼ºï¼Œè§‚å¯Ÿæµ‹è¯•å‡†ç¡®ç‡çš„å˜åŒ–ã€‚

### ç»ƒä¹  2ï¼šEarly Stopping å®è·µ

å®ç°å®Œæ•´çš„æ—©åœè®­ç»ƒå¾ªç¯ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡ï¼Œè®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æƒé‡è¯„ä¼°ã€‚
