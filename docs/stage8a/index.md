---
title: "第八A阶段：大模型原理与微调"
sidebar_position: 0
description: "深入理解大语言模型 LLM 原理，学习 Transformer 架构、注意力机制、LoRA 微调和 RLHF 对齐技术。"
keywords: [大语言模型, LLM, Transformer, LoRA, 微调, RLHF, GPT, 注意力机制]
---

# 第八A阶段：大模型原理与微调

| 信息 | 说明 |
|---|---|
| **预估学时** | 90～120 小时 |
| **前置要求** | 完成第五阶段，强烈建议完成第七阶段 |

## 阶段概述

深入理解大语言模型原理，掌握 Prompt 工程、微调与对齐技术


