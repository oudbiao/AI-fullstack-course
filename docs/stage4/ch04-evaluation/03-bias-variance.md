---
title: "4.3 åå·®-æ–¹å·®æƒè¡¡"
sidebar_position: 12
description: "ç†è§£æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆçš„æœ¬è´¨ï¼ŒæŒæ¡å­¦ä¹ æ›²çº¿å’ŒéªŒè¯æ›²çº¿åˆ†æï¼Œç†è§£æ­£åˆ™åŒ–å¯¹åå·®-æ–¹å·®çš„å½±å“"
keywords: [åå·®, æ–¹å·®, è¿‡æ‹Ÿåˆ, æ¬ æ‹Ÿåˆ, å­¦ä¹ æ›²çº¿, éªŒè¯æ›²çº¿, æ­£åˆ™åŒ–]
---

# åå·®-æ–¹å·®æƒè¡¡

:::tip æœ¬èŠ‚å®šä½
**åå·®-æ–¹å·®æƒè¡¡ï¼ˆBias-Variance Tradeoffï¼‰** æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€é‡è¦çš„ç†è®ºæ¡†æ¶ä¹‹ä¸€ã€‚å®ƒè§£é‡Šäº†ä¸ºä»€ä¹ˆæ¨¡å‹ä¼šæ¬ æ‹Ÿåˆæˆ–è¿‡æ‹Ÿåˆï¼Œä»¥åŠå¦‚ä½•æ‰¾åˆ°ä¸¤è€…ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚
:::

## å­¦ä¹ ç›®æ ‡

- æ·±å…¥ç†è§£åå·®ï¼ˆBiasï¼‰å’Œæ–¹å·®ï¼ˆVarianceï¼‰
- ç†è§£æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆçš„æœ¬è´¨
- æŒæ¡å­¦ä¹ æ›²çº¿åˆ†æ
- æŒæ¡éªŒè¯æ›²çº¿åˆ†æ
- ç†è§£æ­£åˆ™åŒ–å¦‚ä½•å½±å“åå·®-æ–¹å·®

---

## ä¸€ã€ä»€ä¹ˆæ˜¯åå·®å’Œæ–¹å·®ï¼Ÿ

### 1.1 ç›´è§‰ç†è§£â€”â€”æ‰“é¶æ¯”å–»

```mermaid
flowchart LR
    subgraph ä½åå·®ä½æ–¹å·®
        A["ğŸ¯ æ¯æ¬¡éƒ½æ‰“ä¸­é¶å¿ƒ<br/>ï¼ˆç†æƒ³æ¨¡å‹ï¼‰"]
    end
    subgraph ä½åå·®é«˜æ–¹å·®
        B["ğŸ¯ å¹³å‡åœ¨é¶å¿ƒ<br/>ä½†æ•£å¸ƒå¾ˆå¤§<br/>ï¼ˆè¿‡æ‹Ÿåˆï¼‰"]
    end
    subgraph é«˜åå·®ä½æ–¹å·®
        C["ğŸ¯ æ€»æ˜¯åç¦»é¶å¿ƒ<br/>ä½†å¾ˆé›†ä¸­<br/>ï¼ˆæ¬ æ‹Ÿåˆï¼‰"]
    end
    subgraph é«˜åå·®é«˜æ–¹å·®
        D["ğŸ¯ åˆååˆæ•£<br/>ï¼ˆæœ€å·®ï¼‰"]
    end
```

| | åå·®ï¼ˆBiasï¼‰ | æ–¹å·®ï¼ˆVarianceï¼‰ |
|---|-------------|-----------------|
| å«ä¹‰ | æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„ç³»ç»Ÿæ€§åç§» | æ¨¡å‹å¯¹ä¸åŒè®­ç»ƒæ•°æ®çš„æ•æ„Ÿç¨‹åº¦ |
| é«˜ â†’ | æ¬ æ‹Ÿåˆï¼ˆæ¨¡å‹å¤ªç®€å•ï¼‰ | è¿‡æ‹Ÿåˆï¼ˆæ¨¡å‹å¤ªå¤æ‚ï¼‰ |
| è§£å†³ | å¢åŠ æ¨¡å‹å¤æ‚åº¦ | å‡å°‘æ¨¡å‹å¤æ‚åº¦ã€å¢åŠ æ•°æ® |

### 1.2 æ€»è¯¯å·®åˆ†è§£

> **æ€»è¯¯å·® = åå·®Â² + æ–¹å·® + ä¸å¯çº¦è¯¯å·®ï¼ˆå™ªå£°ï¼‰**

```python
import numpy as np
import matplotlib.pyplot as plt

# å¯è§†åŒ–åå·®-æ–¹å·®æƒè¡¡
complexity = np.linspace(0.1, 10, 100)
bias_sq = 5 / complexity
variance = 0.5 * complexity
noise = 0.5 * np.ones_like(complexity)
total = bias_sq + variance + noise

plt.figure(figsize=(8, 5))
plt.plot(complexity, bias_sq, 'b-', linewidth=2, label='åå·®Â²')
plt.plot(complexity, variance, 'r-', linewidth=2, label='æ–¹å·®')
plt.plot(complexity, noise, 'g--', linewidth=1, label='å™ªå£°ï¼ˆä¸å¯çº¦ï¼‰')
plt.plot(complexity, total, 'k-', linewidth=2, label='æ€»è¯¯å·®')

best_idx = np.argmin(total)
plt.axvline(x=complexity[best_idx], color='orange', linestyle=':', label='æœ€ä¼˜å¤æ‚åº¦')

plt.xlabel('æ¨¡å‹å¤æ‚åº¦')
plt.ylabel('è¯¯å·®')
plt.title('åå·®-æ–¹å·®æƒè¡¡')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

## äºŒã€å®é™…è§‚å¯Ÿåå·®å’Œæ–¹å·®

### 2.1 ç”¨å¤šé¡¹å¼å›å½’æ¼”ç¤º

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

# ç”Ÿæˆéçº¿æ€§æ•°æ®
np.random.seed(42)
n = 30
X = np.sort(np.random.uniform(-3, 3, n))
y_true_func = lambda x: np.sin(x)
y = y_true_func(X) + np.random.randn(n) * 0.3

x_plot = np.linspace(-3.5, 3.5, 200)

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
configs = [
    (1, 'æ¬ æ‹Ÿåˆï¼ˆdegree=1ï¼‰\né«˜åå·®ï¼Œä½æ–¹å·®'),
    (4, 'åˆšå¥½ï¼ˆdegree=4ï¼‰\nåå·®æ–¹å·®å¹³è¡¡'),
    (15, 'è¿‡æ‹Ÿåˆï¼ˆdegree=15ï¼‰\nä½åå·®ï¼Œé«˜æ–¹å·®'),
]

for ax, (deg, title) in zip(axes, configs):
    # ç”¨ä¸åŒæ•°æ®å­é›†è®­ç»ƒå¤šæ¬¡ï¼Œè§‚å¯Ÿæ–¹å·®
    for seed in range(10):
        np.random.seed(seed)
        X_sample = np.sort(np.random.uniform(-3, 3, n))
        y_sample = y_true_func(X_sample) + np.random.randn(n) * 0.3

        model = make_pipeline(PolynomialFeatures(deg, include_bias=False), LinearRegression())
        model.fit(X_sample.reshape(-1, 1), y_sample)
        y_pred = model.predict(x_plot.reshape(-1, 1))
        y_pred = np.clip(y_pred, -3, 3)
        ax.plot(x_plot, y_pred, alpha=0.3, color='steelblue')

    ax.plot(x_plot, y_true_func(x_plot), 'r--', linewidth=2, label='çœŸå®å‡½æ•°')
    ax.scatter(X, y, color='black', s=20, zorder=5)
    ax.set_title(title)
    ax.set_ylim(-3, 3)
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)

plt.suptitle('åå·®-æ–¹å·®ç›´è§‰ï¼ˆ10 æ¬¡ä¸åŒæ•°æ®è®­ç»ƒï¼‰', fontsize=13)
plt.tight_layout()
plt.show()
```

:::note è§‚å¯Ÿè¦ç‚¹
- **degree=1**ï¼š10 æ¡çº¿å‡ ä¹é‡åˆï¼ˆä½æ–¹å·®ï¼‰ï¼Œä½†éƒ½åç¦»çœŸå®å‡½æ•°ï¼ˆé«˜åå·®ï¼‰
- **degree=15**ï¼š10 æ¡çº¿å·®å¼‚å¾ˆå¤§ï¼ˆé«˜æ–¹å·®ï¼‰ï¼Œä½†å¹³å‡æ›´æ¥è¿‘çœŸå®ï¼ˆä½åå·®ï¼‰
- **degree=4**ï¼š10 æ¡çº¿è¾ƒä¸€è‡´ï¼ˆé€‚å½“æ–¹å·®ï¼‰ï¼Œä¸”æ¥è¿‘çœŸå®å‡½æ•°ï¼ˆé€‚å½“åå·®ï¼‰
:::

---

## ä¸‰ã€å­¦ä¹ æ›²çº¿

### 3.1 ä»€ä¹ˆæ˜¯å­¦ä¹ æ›²çº¿ï¼Ÿ

å­¦ä¹ æ›²çº¿å±•ç¤º**è®­ç»ƒé›†å¤§å°**å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å®ƒèƒ½å‘Šè¯‰ä½ ï¼š
- æ¨¡å‹æ˜¯æ¬ æ‹Ÿåˆè¿˜æ˜¯è¿‡æ‹Ÿåˆ
- å¢åŠ æ•°æ®æ˜¯å¦æœ‰å¸®åŠ©

```python
from sklearn.model_selection import learning_curve
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_digits

digits = load_digits()
X, y = digits.data, digits.target

def plot_learning_curve(model, X, y, title, ax):
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=5,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='accuracy', n_jobs=-1
    )

    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)

    ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
    ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
    ax.plot(train_sizes, train_mean, 'bo-', label='è®­ç»ƒé›†')
    ax.plot(train_sizes, val_mean, 'ro-', label='éªŒè¯é›†')
    ax.set_xlabel('è®­ç»ƒæ ·æœ¬æ•°')
    ax.set_ylabel('å‡†ç¡®ç‡')
    ax.set_title(title)
    ax.legend()
    ax.grid(True, alpha=0.3)

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# æ¬ æ‹Ÿåˆæ¨¡å‹
plot_learning_curve(
    DecisionTreeClassifier(max_depth=1, random_state=42),
    X, y, 'æ¬ æ‹Ÿåˆï¼ˆmax_depth=1ï¼‰\nè®­ç»ƒå’ŒéªŒè¯éƒ½ä½', axes[0]
)

# åˆšå¥½çš„æ¨¡å‹
plot_learning_curve(
    DecisionTreeClassifier(max_depth=10, random_state=42),
    X, y, 'é€‚å½“å¤æ‚åº¦ï¼ˆmax_depth=10ï¼‰', axes[1]
)

# è¿‡æ‹Ÿåˆæ¨¡å‹
plot_learning_curve(
    DecisionTreeClassifier(max_depth=None, random_state=42),
    X, y, 'è¿‡æ‹Ÿåˆï¼ˆmax_depth=Noneï¼‰\nè®­ç»ƒå’ŒéªŒè¯å·®è·å¤§', axes[2]
)

plt.tight_layout()
plt.show()
```

### 3.2 å¦‚ä½•è§£è¯»å­¦ä¹ æ›²çº¿

| ç°è±¡ | è¯Šæ–­ | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| è®­ç»ƒå’ŒéªŒè¯éƒ½ä½ | **æ¬ æ‹Ÿåˆ** | å¢åŠ æ¨¡å‹å¤æ‚åº¦ |
| è®­ç»ƒé«˜ï¼ŒéªŒè¯ä½ | **è¿‡æ‹Ÿåˆ** | æ›´å¤šæ•°æ® / æ­£åˆ™åŒ– / ç®€åŒ–æ¨¡å‹ |
| ä¸¤æ¡çº¿æ”¶æ•›ä¸”éƒ½é«˜ | **åˆšå¥½** | æ¨¡å‹ä¸é”™ |
| éªŒè¯è¿˜åœ¨ä¸Šå‡ | éœ€è¦æ›´å¤šæ•°æ® | æ”¶é›†æ›´å¤šæ•°æ® |

---

## å››ã€éªŒè¯æ›²çº¿

### 4.1 ä»€ä¹ˆæ˜¯éªŒè¯æ›²çº¿ï¼Ÿ

éªŒè¯æ›²çº¿å±•ç¤º**æŸä¸ªè¶…å‚æ•°**å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¸®ä½ æ‰¾åˆ°æœ€ä¼˜å€¼ã€‚

```python
from sklearn.model_selection import validation_curve

# max_depth å¯¹å†³ç­–æ ‘çš„å½±å“
param_range = range(1, 25)
train_scores, val_scores = validation_curve(
    DecisionTreeClassifier(random_state=42), X, y,
    param_name='max_depth', param_range=param_range,
    cv=5, scoring='accuracy', n_jobs=-1
)

train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

plt.figure(figsize=(8, 5))
plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
plt.plot(param_range, train_mean, 'bo-', label='è®­ç»ƒé›†')
plt.plot(param_range, val_mean, 'ro-', label='éªŒè¯é›†')
plt.xlabel('max_depth')
plt.ylabel('å‡†ç¡®ç‡')
plt.title('éªŒè¯æ›²çº¿ï¼šmax_depth çš„å½±å“')
plt.legend()
plt.grid(True, alpha=0.3)

best_depth = param_range[np.argmax(val_mean)]
plt.axvline(x=best_depth, color='green', linestyle='--', label=f'æœ€ä¼˜ depth={best_depth}')
plt.legend()
plt.show()
```

### 4.2 å¦‚ä½•è§£è¯»éªŒè¯æ›²çº¿

```mermaid
flowchart LR
    subgraph éªŒè¯æ›²çº¿
        L["â† æ¬ æ‹ŸåˆåŒº<br/>ï¼ˆå‚æ•°å¤ªå°ï¼‰"]
        M["æœ€ä¼˜ç‚¹<br/>ï¼ˆéªŒè¯åˆ†æ•°æœ€é«˜ï¼‰"]
        R["è¿‡æ‹ŸåˆåŒº â†’<br/>ï¼ˆå‚æ•°å¤ªå¤§ï¼‰"]
    end
    L --> M --> R

    style L fill:#ffebee,stroke:#c62828,color:#333
    style M fill:#e8f5e9,stroke:#2e7d32,color:#333
    style R fill:#ffebee,stroke:#c62828,color:#333
```

---

## äº”ã€æ­£åˆ™åŒ–å¯¹åå·®-æ–¹å·®çš„å½±å“

```python
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score

# éçº¿æ€§æ•°æ®
np.random.seed(42)
X_nl = np.sort(np.random.uniform(-3, 3, 100)).reshape(-1, 1)
y_nl = np.sin(X_nl.ravel()) + np.random.randn(100) * 0.3

# é«˜é˜¶å¤šé¡¹å¼ + ä¸åŒæ­£åˆ™åŒ–å¼ºåº¦
alphas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]
train_scores = []
cv_scores = []

for alpha in alphas:
    model = make_pipeline(
        StandardScaler(),
        PolynomialFeatures(degree=10, include_bias=False),
        Ridge(alpha=alpha)
    )
    model.fit(X_nl, y_nl)
    train_scores.append(model.score(X_nl, y_nl))

    cv = cross_val_score(model, X_nl, y_nl, cv=5)
    cv_scores.append(cv.mean())

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# alpha vs åˆ†æ•°
axes[0].plot(alphas, train_scores, 'bo-', label='è®­ç»ƒé›†')
axes[0].plot(alphas, cv_scores, 'ro-', label='CV éªŒè¯é›†')
axes[0].set_xscale('log')
axes[0].set_xlabel('æ­£åˆ™åŒ–å¼ºåº¦ Î±')
axes[0].set_ylabel('RÂ² åˆ†æ•°')
axes[0].set_title('æ­£åˆ™åŒ–å¼ºåº¦ vs æ¨¡å‹è¡¨ç°')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# æ‹Ÿåˆæ›²çº¿å¯¹æ¯”
x_plot = np.linspace(-3.5, 3.5, 200).reshape(-1, 1)
for alpha, color, ls in [(0.0001, 'blue', '--'), (0.1, 'green', '-'), (100, 'orange', ':')]:
    model = make_pipeline(
        StandardScaler(),
        PolynomialFeatures(degree=10, include_bias=False),
        Ridge(alpha=alpha)
    )
    model.fit(X_nl, y_nl)
    y_pred = model.predict(x_plot)
    axes[1].plot(x_plot, np.clip(y_pred, -3, 3), color=color, linestyle=ls,
                  linewidth=2, label=f'Î±={alpha}')

axes[1].scatter(X_nl, y_nl, s=15, alpha=0.5, color='gray')
axes[1].plot(x_plot, np.sin(x_plot), 'r--', linewidth=1, label='çœŸå®å‡½æ•°')
axes[1].set_title('ä¸åŒæ­£åˆ™åŒ–å¼ºåº¦çš„æ‹Ÿåˆæ•ˆæœ')
axes[1].set_ylim(-3, 3)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

| Î± å€¼ | åå·® | æ–¹å·® | çŠ¶æ€ |
|------|------|------|------|
| å¾ˆå°ï¼ˆ0.0001ï¼‰ | ä½ | é«˜ | è¿‡æ‹Ÿåˆ |
| é€‚ä¸­ï¼ˆ0.1ï¼‰ | é€‚ä¸­ | é€‚ä¸­ | åˆšå¥½ |
| å¾ˆå¤§ï¼ˆ100ï¼‰ | é«˜ | ä½ | æ¬ æ‹Ÿåˆ |

---

## å…­ã€å®ç”¨è¯Šæ–­æµç¨‹

```mermaid
flowchart TD
    A["æ¨¡å‹è¡¨ç°ä¸å¥½"] --> B{"è®­ç»ƒé›†è¡¨ç°å¦‚ä½•ï¼Ÿ"}
    B -->|"è®­ç»ƒé›†ä¹Ÿä¸å¥½"| UF["æ¬ æ‹Ÿåˆ"]
    B -->|"è®­ç»ƒé›†å¾ˆå¥½"| OF["è¿‡æ‹Ÿåˆ"]

    UF --> U1["å¢åŠ ç‰¹å¾"]
    UF --> U2["ç”¨æ›´å¤æ‚çš„æ¨¡å‹"]
    UF --> U3["å‡å°æ­£åˆ™åŒ–"]

    OF --> O1["å¢åŠ è®­ç»ƒæ•°æ®"]
    OF --> O2["å¢åŠ æ­£åˆ™åŒ–"]
    OF --> O3["å‡å°‘ç‰¹å¾"]
    OF --> O4["ç”¨æ›´ç®€å•çš„æ¨¡å‹"]

    style UF fill:#fff3e0,stroke:#e65100,color:#333
    style OF fill:#ffebee,stroke:#c62828,color:#333
```

---

## ä¸ƒã€å°ç»“

| è¦ç‚¹ | è¯´æ˜ |
|------|------|
| åå·® | æ¨¡å‹çš„ç³»ç»Ÿæ€§è¯¯å·®ï¼Œæ¨¡å‹å¤ªç®€å•å¯¼è‡´ |
| æ–¹å·® | æ¨¡å‹å¯¹æ•°æ®å˜åŒ–çš„æ•æ„Ÿåº¦ï¼Œæ¨¡å‹å¤ªå¤æ‚å¯¼è‡´ |
| æƒè¡¡ | å‡å°‘åå·®é€šå¸¸å¢åŠ æ–¹å·®ï¼Œåä¹‹äº¦ç„¶ |
| å­¦ä¹ æ›²çº¿ | è®­ç»ƒé›†å¤§å° vs è¡¨ç°ï¼Œè¯Šæ–­æ¬ æ‹Ÿåˆ/è¿‡æ‹Ÿåˆ |
| éªŒè¯æ›²çº¿ | è¶…å‚æ•° vs è¡¨ç°ï¼Œæ‰¾æœ€ä¼˜å€¼ |
| æ­£åˆ™åŒ– | å¢å¤§ Î± â†’ å¢å¤§åå·®ã€å‡å°æ–¹å·® |

:::info è¿æ¥åç»­
- **ä¸‹ä¸€èŠ‚**ï¼šè¶…å‚æ•°è°ƒä¼˜â€”â€”ç³»ç»ŸåŒ–æœç´¢æœ€ä¼˜å‚æ•°
:::

---

## åŠ¨æ‰‹ç»ƒä¹ 

### ç»ƒä¹  1ï¼šå­¦ä¹ æ›²çº¿è¯Šæ–­

ç”¨ `load_digits()` åˆ†åˆ«ç”»éšæœºæ£®æ—ï¼ˆn_estimators=100ï¼‰å’Œé€»è¾‘å›å½’çš„å­¦ä¹ æ›²çº¿ã€‚å“ªä¸ªæ›´å€¾å‘è¿‡æ‹Ÿåˆï¼Ÿ

### ç»ƒä¹  2ï¼šéªŒè¯æ›²çº¿

ç”¨ `load_wine()` ç”»éšæœºæ£®æ— `n_estimators`ï¼ˆ10~500ï¼‰çš„éªŒè¯æ›²çº¿ï¼Œæ‰¾åˆ°æœ€ä¼˜æ ‘æ•°é‡ã€‚

### ç»ƒä¹  3ï¼šæ­£åˆ™åŒ–å®éªŒ

ç”¨å¤šé¡¹å¼å›å½’ï¼ˆdegree=15ï¼‰+ Ridge å›å½’ï¼Œç”» alphaï¼ˆä» 0.0001 åˆ° 1000ï¼‰çš„éªŒè¯æ›²çº¿ã€‚åœ¨åŒä¸€å¼ å›¾ä¸Šæ ‡æ³¨æ¬ æ‹ŸåˆåŒºå’Œè¿‡æ‹ŸåˆåŒºã€‚
