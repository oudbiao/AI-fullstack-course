---
title: "2.3 å†³ç­–æ ‘"
sidebar_position: 5
description: "ç†è§£å†³ç­–æ ‘çš„æ„å»ºè¿‡ç¨‹ã€ä¿¡æ¯å¢ç›Šä¸åŸºå°¼æŒ‡æ•°ã€å‰ªæç­–ç•¥ã€å†³ç­–æ ‘å¯è§†åŒ–ï¼Œä»¥åŠå›å½’æ ‘"
keywords: [å†³ç­–æ ‘, ä¿¡æ¯å¢ç›Š, åŸºå°¼æŒ‡æ•°, å‰ªæ, å›å½’æ ‘, CART, å¯è§£é‡Šæ€§]
---

# å†³ç­–æ ‘

:::tip æœ¬èŠ‚å®šä½
å†³ç­–æ ‘æ˜¯**æœ€ç›´è§‰ã€æœ€æ˜“è§£é‡Š**çš„ ML ç®—æ³•ã€‚å®ƒå°±åƒä¸€ä¸ª"20 ä¸ªé—®é¢˜"æ¸¸æˆï¼šé€šè¿‡ä¸€è¿ä¸²çš„æ˜¯/å¦åˆ¤æ–­ï¼ŒæŠŠæ•°æ®åˆ†ç±»ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå†³ç­–æ ‘æ˜¯åé¢é›†æˆå­¦ä¹ ï¼ˆéšæœºæ£®æ—ã€XGBoostï¼‰çš„åŸºç¡€ã€‚
:::

## å­¦ä¹ ç›®æ ‡

- ç†è§£å†³ç­–æ ‘çš„æ„å»ºè¿‡ç¨‹
- æŒæ¡ä¿¡æ¯å¢ç›Šã€åŸºå°¼æŒ‡æ•°ï¼ˆä¸ç¬¬ä¸‰é˜¶æ®µç†µçš„æ¦‚å¿µè¡”æ¥ï¼‰
- ç†è§£å‰ªæç­–ç•¥ï¼ˆé¢„å‰ªæã€åå‰ªæï¼‰
- æŒæ¡å†³ç­–æ ‘çš„å¯è§†åŒ–ä¸è§£é‡Šæ€§
- äº†è§£å›å½’æ ‘

---

## ä¸€ã€å†³ç­–æ ‘çš„ç›´è§‰

### 1.1 ç”Ÿæ´»ä¸­çš„å†³ç­–æ ‘

```mermaid
flowchart TD
    A["ä»Šå¤©å‡ºå»ç©å—ï¼Ÿ"] --> B{"å¤©æ°”å¥½ï¼Ÿ"}
    B -->|"æ˜¯"| C{"æœ‰ç©ºï¼Ÿ"}
    B -->|"å¦"| D["ä¸å‡ºå» ğŸ "]
    C -->|"æ˜¯"| E["å‡ºå»ç© ğŸ‰"]
    C -->|"å¦"| F["ä¸å‡ºå» ğŸ "]

    style A fill:#e3f2fd,stroke:#1565c0,color:#333
    style E fill:#e8f5e9,stroke:#2e7d32,color:#333
    style D fill:#ffebee,stroke:#c62828,color:#333
    style F fill:#ffebee,stroke:#c62828,color:#333
```

å†³ç­–æ ‘å°±æ˜¯ä¸€ç³»åˆ—çš„**if-else åˆ¤æ–­**ï¼Œæ¯æ¬¡æ ¹æ®ä¸€ä¸ªç‰¹å¾çš„å€¼æŠŠæ•°æ®åˆ†æˆä¸¤ï¼ˆæˆ–å¤šï¼‰ç»„ã€‚

### 1.2 æœºå™¨å­¦ä¹ ä¸­çš„å†³ç­–æ ‘

| è¦ç´  | è¯´æ˜ |
|------|------|
| **æ ¹èŠ‚ç‚¹** | æœ€é¡¶éƒ¨çš„èŠ‚ç‚¹ï¼ŒåŒ…å«æ‰€æœ‰æ•°æ® |
| **å†…éƒ¨èŠ‚ç‚¹** | åšåˆ¤æ–­çš„èŠ‚ç‚¹ï¼ˆæŒ‰æŸä¸ªç‰¹å¾åˆ†è£‚ï¼‰ |
| **å¶èŠ‚ç‚¹** | æœ€ç»ˆçš„å†³ç­–ç»“æœï¼ˆç±»åˆ«æˆ–æ•°å€¼ï¼‰ |
| **åˆ†è£‚æ¡ä»¶** | å¦‚"èŠ±ç“£é•¿åº¦ â‰¤ 2.5cm" |
| **æ·±åº¦** | ä»æ ¹åˆ°å¶çš„æœ€é•¿è·¯å¾„ |

### 1.3 ä¸€ä¸ªç®€å•ä¾‹å­

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# åªç”¨ 2 ä¸ªç‰¹å¾ï¼Œæ–¹ä¾¿å¯è§†åŒ–
iris = load_iris()
X = iris.data[:, 2:4]  # èŠ±ç“£é•¿åº¦å’Œå®½åº¦
y = iris.target

# è®­ç»ƒä¸€æ£µæµ…å±‚å†³ç­–æ ‘
tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X, y)

# å¯è§†åŒ–å†³ç­–æ ‘
fig, ax = plt.subplots(figsize=(14, 8))
plot_tree(tree, feature_names=['èŠ±ç“£é•¿åº¦', 'èŠ±ç“£å®½åº¦'],
          class_names=iris.target_names, filled=True,
          rounded=True, fontsize=10, ax=ax)
plt.title('é¸¢å°¾èŠ±å†³ç­–æ ‘ï¼ˆmax_depth=3ï¼‰')
plt.tight_layout()
plt.show()
```

---

## äºŒã€å†³ç­–æ ‘å¦‚ä½•"å­¦ä¹ "ï¼Ÿâ€”â€”åˆ†è£‚å‡†åˆ™

### 2.1 æ ¸å¿ƒé—®é¢˜

æ¯ä¸ªèŠ‚ç‚¹ä¸Šï¼Œç®—æ³•éœ€è¦å†³å®šï¼š
1. **ç”¨å“ªä¸ªç‰¹å¾**åˆ†è£‚ï¼Ÿ
2. **ç”¨ä»€ä¹ˆé˜ˆå€¼**åˆ†è£‚ï¼Ÿ

ç›®æ ‡ï¼šè®©æ¯æ¬¡åˆ†è£‚åï¼Œå­èŠ‚ç‚¹çš„æ•°æ®å°½å¯èƒ½**"çº¯"**ï¼ˆåŒç±»æ•°æ®èšåœ¨ä¸€èµ·ï¼‰ã€‚

### 2.2 ä¿¡æ¯å¢ç›Šä¸ç†µ

:::info ä¸ç¬¬ä¸‰é˜¶æ®µçš„è¡”æ¥
ä½ åœ¨ç¬¬ä¸‰é˜¶æ®µ"2.4 ä¿¡æ¯è®ºåŸºç¡€"ä¸­å­¦è¿‡**ç†µ**â€”â€”å®ƒè¡¡é‡ä¸€ä¸ªé›†åˆçš„"ä¸ç¡®å®šæ€§"ã€‚å†³ç­–æ ‘å°±æ˜¯ç”¨ç†µæ¥å†³å®šå¦‚ä½•åˆ†è£‚ã€‚
:::

**ç†µï¼ˆEntropyï¼‰**ï¼š

> **H(S) = -Î£ pk Ã— logâ‚‚(pk)**

- `pk` = ç±»åˆ« k åœ¨é›†åˆ S ä¸­çš„æ¯”ä¾‹
- ç†µè¶Šå¤§ = è¶Š"æ··ä¹±"ï¼›ç†µ = 0 = å®Œå…¨çº¯ï¼ˆåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼‰

**ä¿¡æ¯å¢ç›Š**ï¼šåˆ†è£‚å‰åç†µçš„å‡å°‘é‡ã€‚

> **IG(S, A) = H(S) - Î£ (|Sv|/|S|) Ã— H(Sv)**

```python
import numpy as np

def entropy(y):
    """è®¡ç®—ç†µ"""
    classes, counts = np.unique(y, return_counts=True)
    probs = counts / len(y)
    return -np.sum(probs * np.log2(probs + 1e-10))

def information_gain(y, y_left, y_right):
    """è®¡ç®—ä¿¡æ¯å¢ç›Š"""
    n = len(y)
    return entropy(y) - (len(y_left)/n * entropy(y_left) + len(y_right)/n * entropy(y_right))

# ç¤ºä¾‹ï¼š10 ä¸ªæ ·æœ¬
y_parent = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])  # 5:5 æ··åˆ
print(f"çˆ¶èŠ‚ç‚¹ç†µ: {entropy(y_parent):.4f}")

# åˆ†è£‚æ–¹æ¡ˆ Aï¼šå®Œç¾åˆ†è£‚
y_left_a = np.array([0, 0, 0, 0, 0])  # å…¨ 0
y_right_a = np.array([1, 1, 1, 1, 1])  # å…¨ 1
ig_a = information_gain(y_parent, y_left_a, y_right_a)
print(f"æ–¹æ¡ˆ Aï¼ˆå®Œç¾åˆ†è£‚ï¼‰ä¿¡æ¯å¢ç›Š: {ig_a:.4f}")

# åˆ†è£‚æ–¹æ¡ˆ Bï¼šå¾ˆå·®çš„åˆ†è£‚
y_left_b = np.array([0, 0, 1, 1, 1])   # 2:3 æ··åˆ
y_right_b = np.array([0, 0, 0, 1, 1])   # 3:2 æ··åˆ
ig_b = information_gain(y_parent, y_left_b, y_right_b)
print(f"æ–¹æ¡ˆ Bï¼ˆå·®çš„åˆ†è£‚ï¼‰ä¿¡æ¯å¢ç›Š: {ig_b:.4f}")
```

### 2.3 åŸºå°¼æŒ‡æ•°ï¼ˆGini Impurityï¼‰

å¦ä¸€ç§è¡¡é‡"çº¯åº¦"çš„æŒ‡æ ‡ï¼Œè®¡ç®—æ›´å¿«ï¼š

> **Gini(S) = 1 - Î£ pkÂ²**

- Gini = 0 â†’ å®Œå…¨çº¯
- Gini æœ€å¤§ â†’ å®Œå…¨æ··ä¹±

```python
def gini(y):
    """è®¡ç®—åŸºå°¼æŒ‡æ•°"""
    classes, counts = np.unique(y, return_counts=True)
    probs = counts / len(y)
    return 1 - np.sum(probs ** 2)

# å¯¹æ¯”ç†µå’ŒåŸºå°¼æŒ‡æ•°
p = np.linspace(0.01, 0.99, 100)
entropy_vals = -p * np.log2(p) - (1-p) * np.log2(1-p)
gini_vals = 2 * p * (1 - p)

plt.figure(figsize=(8, 5))
plt.plot(p, entropy_vals, 'b-', linewidth=2, label='ç†µ (Entropy)')
plt.plot(p, gini_vals, 'r-', linewidth=2, label='åŸºå°¼æŒ‡æ•° (Gini)')
plt.xlabel('æ­£ç±»æ¯”ä¾‹ p')
plt.ylabel('ä¸çº¯åº¦')
plt.title('ç†µ vs åŸºå°¼æŒ‡æ•°')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 2.4 sklearn ä¸­çš„é€‰æ‹©

| å‚æ•° | é€‰é¡¹ | è¯´æ˜ |
|------|------|------|
| `criterion='gini'` | åŸºå°¼æŒ‡æ•° | sklearn **é»˜è®¤**ï¼Œè®¡ç®—å¿« |
| `criterion='entropy'` | ä¿¡æ¯å¢ç›Š | åˆ†è£‚æ›´ç²¾ç¡®ï¼Œä½†è®¡ç®—ç¨æ…¢ |

å®é™…ä½¿ç”¨ä¸­ä¸¤è€…å·®å¼‚ä¸å¤§ï¼Œé»˜è®¤ç”¨ `gini` å³å¯ã€‚

---

## ä¸‰ã€å†³ç­–è¾¹ç•Œå¯è§†åŒ–

```python
from sklearn.datasets import make_classification, make_moons
from sklearn.tree import DecisionTreeClassifier
import numpy as np
import matplotlib.pyplot as plt

def plot_decision_boundary(ax, model, X, y, title):
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                          np.linspace(y_min, y_max, 200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=20, edgecolors='w', linewidth=0.5)
    ax.set_title(title)
    ax.grid(True, alpha=0.3)

# ä¸åŒæ·±åº¦çš„å†³ç­–æ ‘
X, y = make_moons(n_samples=300, noise=0.25, random_state=42)

fig, axes = plt.subplots(1, 4, figsize=(18, 4))
depths = [1, 3, 5, None]

for ax, depth in zip(axes, depths):
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X, y)
    label = f'æ·±åº¦ä¸é™' if depth is None else f'æ·±åº¦={depth}'
    plot_decision_boundary(ax, tree, X, y,
                          f'{label}\nè®­ç»ƒå‡†ç¡®ç‡: {tree.score(X, y):.1%}')

plt.suptitle('å†³ç­–æ ‘æ·±åº¦å¯¹å†³ç­–è¾¹ç•Œçš„å½±å“', fontsize=13)
plt.tight_layout()
plt.show()
```

:::warning å†³ç­–æ ‘çš„è¿‡æ‹Ÿåˆ
ä¸é™æ·±åº¦çš„å†³ç­–æ ‘ä¼šæŠŠæ¯ä¸ªè®­ç»ƒæ ·æœ¬éƒ½"è®°ä½"ï¼ˆè®­ç»ƒå‡†ç¡®ç‡ 100%ï¼‰ï¼Œä½†å†³ç­–è¾¹ç•Œä¼šéå¸¸å¤æ‚ã€‚è¿™å°±æ˜¯è¿‡æ‹Ÿåˆâ€”â€”éœ€è¦é€šè¿‡**å‰ªæ**æ¥æ§åˆ¶ã€‚
:::

---

## å››ã€å‰ªæâ€”â€”æ§åˆ¶å¤æ‚åº¦

### 4.1 é¢„å‰ªæï¼ˆPre-pruningï¼‰

**åœ¨æ„å»ºè¿‡ç¨‹ä¸­**é™åˆ¶æ ‘çš„ç”Ÿé•¿ï¼š

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `max_depth` | æœ€å¤§æ·±åº¦ | Noneï¼ˆä¸é™ï¼‰ |
| `min_samples_split` | èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°æ‰èƒ½åˆ†è£‚ | 2 |
| `min_samples_leaf` | å¶å­èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•° | 1 |
| `max_leaf_nodes` | æœ€å¤§å¶èŠ‚ç‚¹æ•° | Noneï¼ˆä¸é™ï¼‰ |

```python
from sklearn.model_selection import train_test_split

X, y = make_moons(n_samples=500, noise=0.3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# å¯¹æ¯”ä¸åŒæ·±åº¦
fig, axes = plt.subplots(1, 4, figsize=(18, 4))
configs = [
    (None, 'ä¸å‰ªæ'),
    (3, 'max_depth=3'),
    (5, 'max_depth=5'),
    (10, 'max_depth=10'),
]

for ax, (depth, title) in zip(axes, configs):
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X_train, y_train)
    train_acc = tree.score(X_train, y_train)
    test_acc = tree.score(X_test, y_test)
    plot_decision_boundary(ax, tree, X_train, y_train,
                          f'{title}\nè®­ç»ƒ: {train_acc:.1%}, æµ‹è¯•: {test_acc:.1%}')

plt.suptitle('é¢„å‰ªæå¯¹è¿‡æ‹Ÿåˆçš„æ§åˆ¶', fontsize=13)
plt.tight_layout()
plt.show()
```

### 4.2 åå‰ªæï¼ˆPost-pruningï¼‰â€”â€”ä»£ä»·å¤æ‚åº¦å‰ªæ

**å…ˆé•¿æˆå®Œå…¨æ ‘ï¼Œå†å›å¤´"ä¿®å‰ª"**ã€‚sklearn ä½¿ç”¨ `ccp_alpha`ï¼ˆCost Complexity Pruningï¼‰å‚æ•°ã€‚

```python
# æ‰¾åˆ°æœ€ä¼˜çš„ ccp_alpha
tree_full = DecisionTreeClassifier(random_state=42)
tree_full.fit(X_train, y_train)

# è·å–ä¸åŒ alpha å¯¹åº”çš„å­æ ‘
path = tree_full.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas

# å¯¹æ¯ä¸ª alpha è®­ç»ƒä¸€æ£µæ ‘
train_scores = []
test_scores = []
for alpha in ccp_alphas:
    tree = DecisionTreeClassifier(ccp_alpha=alpha, random_state=42)
    tree.fit(X_train, y_train)
    train_scores.append(tree.score(X_train, y_train))
    test_scores.append(tree.score(X_test, y_test))

plt.figure(figsize=(8, 5))
plt.plot(ccp_alphas, train_scores, 'b-o', markersize=3, label='è®­ç»ƒé›†')
plt.plot(ccp_alphas, test_scores, 'r-o', markersize=3, label='æµ‹è¯•é›†')
plt.xlabel('ccp_alpha')
plt.ylabel('å‡†ç¡®ç‡')
plt.title('ä»£ä»·å¤æ‚åº¦å‰ªæ')
plt.legend()
plt.grid(True, alpha=0.3)

# æ ‡æ³¨æœ€ä¼˜ç‚¹
best_idx = np.argmax(test_scores)
plt.axvline(x=ccp_alphas[best_idx], color='green', linestyle='--',
            label=f'æœ€ä¼˜ alpha={ccp_alphas[best_idx]:.4f}')
plt.legend()
plt.show()

print(f"æœ€ä¼˜ ccp_alpha: {ccp_alphas[best_idx]:.4f}")
print(f"æœ€ä¼˜æµ‹è¯•å‡†ç¡®ç‡: {test_scores[best_idx]:.1%}")
```

---

## äº”ã€ç‰¹å¾é‡è¦æ€§

å†³ç­–æ ‘å¤©ç„¶æä¾›**ç‰¹å¾é‡è¦æ€§**â€”â€”è¡¨ç¤ºæ¯ä¸ªç‰¹å¾å¯¹åˆ†ç±»å†³ç­–çš„è´¡çŒ®ç¨‹åº¦ã€‚

```python
from sklearn.datasets import load_wine
from sklearn.tree import DecisionTreeClassifier

wine = load_wine()
X, y = wine.data, wine.target

tree = DecisionTreeClassifier(max_depth=4, random_state=42)
tree.fit(X, y)

# ç‰¹å¾é‡è¦æ€§
importance = tree.feature_importances_
sorted_idx = np.argsort(importance)

plt.figure(figsize=(8, 6))
plt.barh(range(len(sorted_idx)), importance[sorted_idx], color='steelblue')
plt.yticks(range(len(sorted_idx)), np.array(wine.feature_names)[sorted_idx])
plt.xlabel('ç‰¹å¾é‡è¦æ€§')
plt.title('å†³ç­–æ ‘çš„ç‰¹å¾é‡è¦æ€§ï¼ˆWine æ•°æ®é›†ï¼‰')
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## å…­ã€å›å½’æ ‘

å†³ç­–æ ‘ä¸åªèƒ½åšåˆ†ç±»ï¼Œä¹Ÿèƒ½åš**å›å½’**ã€‚

### 6.1 åŸç†

åˆ†ç±»æ ‘çš„å¶èŠ‚ç‚¹è¾“å‡º**ç±»åˆ«**ï¼›å›å½’æ ‘çš„å¶èŠ‚ç‚¹è¾“å‡º**æ•°å€¼**ï¼ˆè¯¥åŒºåŸŸæ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼ï¼‰ã€‚

### 6.2 ç¤ºä¾‹

```python
from sklearn.tree import DecisionTreeRegressor

# ç”Ÿæˆéçº¿æ€§æ•°æ®
np.random.seed(42)
X_reg = np.sort(np.random.uniform(0, 10, 200)).reshape(-1, 1)
y_reg = np.sin(X_reg.ravel()) + np.random.randn(200) * 0.3

# ä¸åŒæ·±åº¦çš„å›å½’æ ‘
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
depths = [2, 5, None]

for ax, depth in zip(axes, depths):
    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)
    tree.fit(X_reg, y_reg)

    X_test_reg = np.linspace(0, 10, 500).reshape(-1, 1)
    y_pred = tree.predict(X_test_reg)

    ax.scatter(X_reg, y_reg, s=10, alpha=0.5, color='steelblue')
    ax.plot(X_test_reg, y_pred, 'r-', linewidth=2)
    label = 'ä¸é™' if depth is None else str(depth)
    ax.set_title(f'æ·±åº¦={label}, RÂ²={tree.score(X_reg, y_reg):.3f}')
    ax.grid(True, alpha=0.3)

plt.suptitle('å›å½’æ ‘çš„ä¸åŒæ·±åº¦', fontsize=13)
plt.tight_layout()
plt.show()
```

:::note å›å½’æ ‘ vs çº¿æ€§å›å½’
å›å½’æ ‘çš„é¢„æµ‹æ˜¯**é˜¶æ¢¯çŠ¶**çš„ï¼ˆæ¯ä¸ªåŒºé—´è¾“å‡ºä¸€ä¸ªå¸¸æ•°ï¼‰ï¼Œè€Œä¸æ˜¯å¹³æ»‘çš„ã€‚å®ƒå¤©ç„¶å¯ä»¥æ‹Ÿåˆéçº¿æ€§æ•°æ®ï¼Œä½†ä¹Ÿå®¹æ˜“è¿‡æ‹Ÿåˆã€‚
:::

---

## ä¸ƒã€å†³ç­–æ ‘çš„ä¼˜ç¼ºç‚¹

| ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|
| æ˜“äºç†è§£å’Œè§£é‡Šï¼ˆå¯è§†åŒ–ï¼‰ | å®¹æ˜“è¿‡æ‹Ÿåˆ |
| ä¸éœ€è¦ç‰¹å¾ç¼©æ”¾ | å¯¹æ•°æ®å¾®å°å˜åŒ–æ•æ„Ÿ |
| å¯å¤„ç†åˆ†ç±»å’Œå›å½’ | å†³ç­–è¾¹ç•Œæ˜¯è½´å¯¹é½çš„ |
| å¯å¤„ç†å¤šç±»åˆ«é—®é¢˜ | è´ªå¿ƒç®—æ³•ï¼Œä¸ä¿è¯å…¨å±€æœ€ä¼˜ |
| éšå¼ç‰¹å¾é€‰æ‹© | å•æ£µæ ‘è¡¨è¾¾èƒ½åŠ›æœ‰é™ |

:::info è§£å†³ç¼ºç‚¹çš„æ–¹æ³•
å†³ç­–æ ‘çš„å¤šæ•°ç¼ºç‚¹å¯ä»¥é€šè¿‡**é›†æˆå­¦ä¹ **ï¼ˆä¸‹ä¸€èŠ‚ï¼‰æ¥è§£å†³ï¼š
- å¤šæ£µæ ‘æŠ•ç¥¨ â†’ å‡å°‘è¿‡æ‹Ÿåˆ
- éšæœºé‡‡æ · â†’ å‡å°‘å¯¹å•ä¸ªæ•°æ®ç‚¹çš„æ•æ„Ÿæ€§
:::

---

## å…«ã€å°ç»“

| è¦ç‚¹ | è¯´æ˜ |
|------|------|
| æ ¸å¿ƒæ€æƒ³ | é€šè¿‡ä¸€ç³»åˆ—åˆ¤æ–­æ¡ä»¶å°†æ•°æ®é€’å½’åˆ†å‰² |
| åˆ†è£‚å‡†åˆ™ | ä¿¡æ¯å¢ç›Šï¼ˆç†µï¼‰æˆ–åŸºå°¼æŒ‡æ•° |
| è¿‡æ‹Ÿåˆæ§åˆ¶ | é¢„å‰ªæï¼ˆé™åˆ¶æ·±åº¦/æ ·æœ¬æ•°ï¼‰æˆ–åå‰ªæï¼ˆccp_alphaï¼‰ |
| å¯è§£é‡Šæ€§ | å¯è§†åŒ–å†³ç­–è·¯å¾„ï¼Œè¾“å‡ºç‰¹å¾é‡è¦æ€§ |
| å›å½’æ ‘ | å¶èŠ‚ç‚¹è¾“å‡ºæ•°å€¼è€Œéç±»åˆ« |

:::info è¿æ¥åç»­
- **ä¸‹ä¸€èŠ‚**ï¼šé›†æˆå­¦ä¹ â€”â€”æŠŠå¤šæ£µå†³ç­–æ ‘ç»„åˆèµ·æ¥ï¼Œæ•ˆæœè¿œè¶…å•æ£µæ ‘
- **ç¬¬ä¸‰é˜¶æ®µå›é¡¾**ï¼šç†µå’Œä¿¡æ¯å¢ç›Šï¼ˆ2.4 èŠ‚ä¿¡æ¯è®ºï¼‰
:::

---

## åŠ¨æ‰‹ç»ƒä¹ 

### ç»ƒä¹  1ï¼šæ‰‹åŠ¨è®¡ç®—ä¿¡æ¯å¢ç›Š

æœ‰ 10 ä¸ªæ ·æœ¬ï¼šæ ‡ç­¾ä¸º `[æ˜¯,æ˜¯,å¦,æ˜¯,å¦,å¦,æ˜¯,æ˜¯,å¦,å¦]`ï¼ˆ5 ä¸ª"æ˜¯"ï¼Œ5 ä¸ª"å¦"ï¼‰ã€‚æŒ‰ç‰¹å¾ A åˆ†è£‚åï¼Œå·¦å­èŠ‚ç‚¹ = `[æ˜¯,æ˜¯,æ˜¯,å¦]`ï¼Œå³å­èŠ‚ç‚¹ = `[å¦,å¦,å¦,å¦,æ˜¯,æ˜¯]`ã€‚æ‰‹åŠ¨è®¡ç®—ä¿¡æ¯å¢ç›Šã€‚

### ç»ƒä¹  2ï¼šæ·±åº¦è°ƒä¼˜

ç”¨ `make_moons` æ•°æ®ï¼ˆnoise=0.3ï¼‰ï¼Œå°è¯•ä¸åŒçš„ `max_depth`ï¼ˆ1~20ï¼‰ï¼Œç”»å‡ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†å‡†ç¡®ç‡çš„å˜åŒ–æ›²çº¿ï¼Œæ‰¾åˆ°æœ€ä¼˜æ·±åº¦ã€‚

### ç»ƒä¹  3ï¼šå›å½’æ ‘ vs çº¿æ€§å›å½’

ç”¨ `y = sin(x) + å™ªå£°` ç”Ÿæˆæ•°æ®ï¼Œåˆ†åˆ«ç”¨ `LinearRegression`ã€`PolynomialFeatures(degree=5) + LinearRegression`ã€`DecisionTreeRegressor(max_depth=5)` ä¸‰ç§æ–¹æ³•æ‹Ÿåˆï¼Œç”»å‡ºå¯¹æ¯”å›¾ã€‚

### ç»ƒä¹  4ï¼šç‰¹å¾é‡è¦æ€§

ç”¨ `load_iris()` è®­ç»ƒå†³ç­–æ ‘ï¼Œç”»å‡ºç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾ã€‚å°è¯•å»æ‰ä¸é‡è¦çš„ç‰¹å¾åé‡æ–°è®­ç»ƒï¼Œçœ‹å‡†ç¡®ç‡æ˜¯å¦ä¸‹é™ã€‚
